{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS287-H AHRI HW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HW2 we will walk through Maximum Entropy inverse reinforcement learning, intent inference, and intent expression in a simple gridworld environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "import numpy as np\n",
    "from enum import IntEnum\n",
    "import itertools as iter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define a simple grid world environment with obstacles. In this world, each grid location can be either free or occupied by an obstacle. The agent starts at a specified start and has to end at one of the specified goal locations. It can move up, down, left, and right, as long as it doesn't leave the grid and it doesn't enter an occupied grid location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the agent grid world.\n",
    "\n",
    "class Actions(IntEnum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "\n",
    "class AgentGridworld(object):\n",
    "    \"\"\"\n",
    "    An X by Y gridworld class for an agent in an environment with obstacles.\n",
    "    \"\"\"\n",
    "    Actions = Actions\n",
    "\n",
    "    def __init__(self, X, Y, obstacles, start, goals):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            X [int] -- The width of this gridworld.\n",
    "            Y [int] -- The height of this gridworld.\n",
    "            start [tuple] -- Starting position specified in coords (x, y).\n",
    "            goals [list of tuple] -- List of goal positions specified in coords (x, y).\n",
    "            obstacles [list] -- List of axis-aligned 2D boxes that represent\n",
    "                obstacles in the environment for the agent. Specified in coords:\n",
    "                [[(lower_x, lower_y), (upper_x, upper_y)], [...]]\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(X, int), X\n",
    "        assert isinstance(Y, int), Y\n",
    "        assert X > 0\n",
    "        assert Y > 0\n",
    "\n",
    "        # Set up variables for Agent Gridworld\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.S = X * Y\n",
    "        self.A = len(Actions)\n",
    "        self.start = start\n",
    "        self.goals = goals\n",
    "\n",
    "        # Set the obstacles in the environment.\n",
    "        self.obstacles = obstacles\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    #### Utility functions ####\n",
    "    ###########################\n",
    "\n",
    "    def traj_construct(self, start, goal):\n",
    "        \"\"\"\n",
    "        Construct all trajectories between a start and goal of the shortest length.\n",
    "        Params:\n",
    "            start [tuple] -- Starting position specified in coords (x, y).\n",
    "            goal [tuple] -- Goal position specified in coords (x, y).\n",
    "        Returns:\n",
    "            trajs [list] -- Trajectories between start and goal in states (s).\n",
    "        \"\"\"\n",
    "        trajs = []\n",
    "        def recurse_actions(s_curr, timestep):\n",
    "            # Recursive action combo construction. Select legal combinations.\n",
    "            if timestep == T-1:\n",
    "                if s_curr == s_goal:\n",
    "                    trajs.append(list(traj))\n",
    "            else:\n",
    "                rand_actions = [a for a in Actions]\n",
    "                random.shuffle(rand_actions)\n",
    "                for a in rand_actions:\n",
    "                    s_prime, illegal = self.transition_helper(s_curr, a)\n",
    "                    if not illegal:\n",
    "                        traj[timestep+1] = s_prime\n",
    "                        recurse_actions(s_prime, timestep+1)\n",
    "\n",
    "        s_start = self.coor_to_state(start[0], start[1])\n",
    "        s_goal = self.coor_to_state(goal[0], goal[1])\n",
    "        T = abs(start[0] - goal[0]) + abs(start[1] - goal[1]) + 1\n",
    "        traj = [None] * T\n",
    "        traj[0] = s_start\n",
    "        recurse_actions(s_start, 0)\n",
    "\n",
    "        return trajs\n",
    "\n",
    "    def transition_helper(self, s, a):\n",
    "        \"\"\"\n",
    "        Given a state and action, apply the transition function to get the next state.\n",
    "        Params:\n",
    "            s [int] -- State.\n",
    "            a [int] -- Action taken.\n",
    "        Returns:\n",
    "            s_prime [int] -- Next state.\n",
    "            illegal [bool] -- Whether the action taken was legal or not.\n",
    "        \"\"\"\n",
    "        x, y = self.state_to_coor(s)\n",
    "        assert 0 <= a < self.A\n",
    "\n",
    "        x_prime, y_prime = x, y\n",
    "        if a == Actions.LEFT:\n",
    "            x_prime = x - 1\n",
    "        elif a == Actions.RIGHT:\n",
    "            x_prime = x + 1\n",
    "        elif a == Actions.DOWN:\n",
    "            y_prime = y + 1\n",
    "        elif a == Actions.UP:\n",
    "            y_prime = y - 1\n",
    "        elif a == Actions.UP_LEFT:\n",
    "            x_prime, y_prime = x - 1, y - 1\n",
    "        elif a == Actions.UP_RIGHT:\n",
    "            x_prime, y_prime = x + 1, y - 1\n",
    "        elif a == Actions.DOWN_LEFT:\n",
    "            x_prime, y_prime = x - 1, y + 1\n",
    "        elif a == Actions.DOWN_RIGHT:\n",
    "            x_prime, y_prime = x + 1, y + 1\n",
    "        elif a == Actions.ABSORB:\n",
    "            pass\n",
    "        else:\n",
    "            raise BaseException(\"undefined action {}\".format(a))\n",
    "\n",
    "        illegal = False\n",
    "        if x_prime < 0 or x_prime >= self.X or y_prime < 0 or y_prime >= self.Y:\n",
    "            illegal = True\n",
    "            s_prime = s\n",
    "        else:\n",
    "            s_prime = self.coor_to_state(x_prime, y_prime)\n",
    "            if self.is_blocked(s_prime):\n",
    "                illegal = True\n",
    "        return s_prime, illegal\n",
    "\n",
    "    def get_action(self, s, sp):\n",
    "        \"\"\"\n",
    "        Given two neighboring waypoints, return action between them.\n",
    "        Params:\n",
    "            s [int] -- First waypoint state.\n",
    "            sp [int] -- Next waypoint state.\n",
    "        Returns:\n",
    "            a [int] -- Action taken.\n",
    "        \"\"\"\n",
    "        x1, y1 = self.state_to_coor(s)\n",
    "        x2, y2 = self.state_to_coor(sp)\n",
    "\n",
    "        if x1 == x2:\n",
    "            if y1 == y2:\n",
    "                return Actions.ABSORB\n",
    "            elif y1 < y2:\n",
    "                return Actions.DOWN\n",
    "            else:\n",
    "                return Actions.UP\n",
    "        elif x1 < x2:\n",
    "            if y1 == y2:\n",
    "                return Actions.RIGHT\n",
    "            elif y1 < y2:\n",
    "                return Actions.DOWN_RIGHT\n",
    "            else:\n",
    "                return Actions.UP_RIGHT\n",
    "        else:\n",
    "            if y1 == y2:\n",
    "                return Actions.LEFT\n",
    "            elif y1 < y2:\n",
    "                return Actions.DOWN_LEFT\n",
    "            else:\n",
    "                return Actions.UP_LEFT\n",
    "    \n",
    "    def is_blocked(self, s):\n",
    "        \"\"\"\n",
    "        Returns True if s is blocked.\n",
    "        By default, state-action pairs that lead to blocked states are illegal.\n",
    "        \"\"\"\n",
    "        if self.obstacles is None:\n",
    "            return False\n",
    "\n",
    "        # Check against internal representation of boxes. \n",
    "        x, y = self.state_to_coor(s)\n",
    "        for box in self.obstacles:\n",
    "            if x >= box[0][0] and x <= box[1][0] and y >= box[1][1] and y <= box[0][1]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def visualize_grid(self):\n",
    "        \"\"\"\n",
    "        Visualize the world with its obstacles.\n",
    "        \"\"\"\n",
    "        self.visualize_demos([])\n",
    "\n",
    "    def visualize_demos(self, demos):\n",
    "        \"\"\"\n",
    "        Visualize the world with its obstacles and given demonstration.\n",
    "        \"\"\"\n",
    "        # Create world with obstacles on the map.\n",
    "        world = 0.5*np.ones((self.Y, self.X))\n",
    "\n",
    "        # Add obstacles in the world in opaque color.\n",
    "        for obstacle in self.obstacles:\n",
    "            lower = obstacle[0]\n",
    "            upper = obstacle[1]\n",
    "            world[upper[1]:lower[1]+1, lower[0]:upper[0]+1] = 1.0\n",
    "\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        plt.imshow(world, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "        # Plot markers for start and goal\n",
    "        plt.scatter(self.start[0], self.start[1], c=\"orange\", marker=\"o\", s=100)\n",
    "        for goal in self.goals:\n",
    "            plt.scatter(goal[0], goal[1], c=\"orange\", marker=\"x\", s=300)\n",
    "        \n",
    "        # Plot demonstrations\n",
    "        for t, demo in enumerate(demos):\n",
    "            demo_x = []\n",
    "            demo_y = []\n",
    "            for s in demo:\n",
    "                x, y = self.state_to_coor(s)\n",
    "                demo_x.append(x)\n",
    "                demo_y.append(y)\n",
    "            step = t/float(len(demos)+1)\n",
    "            col = ((1*step), (0*step), (0*step))\n",
    "            plt.plot(demo_x,demo_y, c=col)\n",
    "\n",
    "        plt.xticks(range(self.X), range(self.X))\n",
    "        plt.yticks(np.arange(-0.5,self.Y+0.5),range(self.Y+1))\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_xticks([])\n",
    "        ax = plt.gca()\n",
    "        plt.minorticks_on\n",
    "        ax.grid(True, which='both', color='black', linestyle='-', linewidth=2)\n",
    "        plt.show(block=False)\n",
    "        \n",
    "\n",
    "    #################################\n",
    "    # Conversion functions\n",
    "    #################################\n",
    "    # Helper functions convert between state number (\"state\") and discrete coordinates (\"coor\").\n",
    "    #\n",
    "    # State number (\"state\"):\n",
    "    # A state `s` is an integer such that 0 <= s < self.S.\n",
    "    #\n",
    "    # Discrete coordinates (\"coor\"):\n",
    "    # `x` is an integer such that 0 <= x < self.X. Increasing `x` corresponds to moving east.\n",
    "    # `y` is an integer such that 0 <= y < self.Y. Increasing `y` corresponds to moving south.\n",
    "    #\n",
    "    #################################\n",
    "\n",
    "    def state_to_coor(self, s):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            s [int] -- The state.\n",
    "        Returns:\n",
    "            x, y -- The discrete coordinates corresponding to s.\n",
    "        \"\"\"\n",
    "        assert isinstance(s, int)\n",
    "        assert 0 <= s < self.S\n",
    "        y = s % self.Y\n",
    "        x = s // self.Y\n",
    "        return x, y\n",
    "\n",
    "    def coor_to_state(self, x, y):\n",
    "        \"\"\"\n",
    "        Convert discrete coordinates into a state, if that state exists.\n",
    "        If no such state exists, raise a ValueError.\n",
    "        Params:\n",
    "            x, y [int] -- The discrete x, y coordinates of the state.\n",
    "        Returns:\n",
    "            s [int] -- The state.\n",
    "        \"\"\"\n",
    "\n",
    "        x, y = int(x), int(y)\n",
    "        if not(0 <= x < self.X):\n",
    "            raise ValueError(x, self.X)\n",
    "        if not (0 <= y < self.Y):\n",
    "            raise ValueError(y, self.Y)\n",
    "\n",
    "        return (x * self.Y) + (y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a 6x8 grid world with 2 obstacles. For now, let's set the start in the lower left corner and a goal in the upper right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build grid world.\n",
    "sim_height = 6\n",
    "sim_width = 8\n",
    "obstacles = [[[1,1], [1,1]] , [[4,3],[4,3]]]\n",
    "start = [0, 5]\n",
    "goals = [[7, 0]]\n",
    "gridworld = AgentGridworld(sim_width, sim_height, obstacles, start, goals)\n",
    "gridworld.visualize_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that will become very important later on is a set of all of the feasible trajectories of fixed length between the start and goal, $\\xi\\in\\Xi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SG_trajs = gridworld.traj_construct(start, goals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Human\n",
    "Now let's create a simulated human to produce some demonstrations that our agent will learn from. The human has access to the grid world, and, therefore, to the trajectories $\\xi\\in\\Xi$ that are feasible in the grid world, but we need to implement a way of selecting amongst those trajectories. We need two components for this: a <b>cost function</b>, and an <b>observation model</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Cost Function\n",
    "The cost function $C : \\Xi \\rightarrow \\mathbb{R}$ maps trajectories to a real-numbered score and defines how much the human prefers some trajectories in the grid world over the others. To make computation tractable, this cost function is typically parameterized by some *weight vector* $\\theta$: $C_\\theta$. While the cost could operate directly on the state trajectory, it is more common (also for tractability's sake) to write it as a function of *features* $\\Phi : \\Xi \\rightarrow \\mathbb{R}^n$ - aspects of behavior that the person might care about. For example, the feature vector of a trajectory $\\Phi(\\xi)$ could be the trajectory's total distance to an obstacle, its jerk, its average $x$ coordinate, etc. In class, we saw the cost written as a linear combination of features $C_\\theta = \\theta^T\\Phi$, and we'll use this same cost structure in the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of $\\theta$ as a parameter that determines how much the human prioritizes certain features over others. We will play with different weight vectors $\\theta$ later on and see how that affects the demonstrations produced. Before we do this, we need to define what features $\\Phi$ matter to the simulated human we're creating. The implementation below already defines some example features; <b>fill in the `dist_to_obstacles` and `dist_to_goals` functions</b>, which should implement an Euclidean distance to all obstacles and goals in the environment, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#### Featurization functions ####\n",
    "#################################\n",
    "\n",
    "####### Obstacles Feature #######\n",
    "\n",
    "def obstacles_feature(traj):\n",
    "    \"\"\"\n",
    "    Compute obstacle feature values for all obstacles and the entire trajectory.\n",
    "    The obstacle feature consists of distance from the obstacle.\n",
    "    Params:\n",
    "        traj [list] -- The trajectory.\n",
    "    Returns:\n",
    "        obstacle_feat [list] -- A list of obstacle features per obstacle.\n",
    "    \"\"\"\n",
    "    obstacle_feat = np.zeros(len(gridworld.obstacles))\n",
    "    for s in traj:\n",
    "        obstacle_feat += np.asarray(dist_to_obstacles(s))\n",
    "    return obstacle_feat.tolist()\n",
    "\n",
    "def dist_to_obstacles(s):\n",
    "    \"\"\"\n",
    "    Compute distance from state s to the obstacles in the environment.\n",
    "    Params:\n",
    "        s [int] -- The state.\n",
    "    Returns:\n",
    "        distances [list] -- The distance to the obstacles in the environment.\n",
    "    \"\"\"\n",
    "    x, y = gridworld.state_to_coor(s)\n",
    "    distances = []\n",
    "    \n",
    "    # Compute delta x and delta y in distance from obstacle\n",
    "    for obstacle in gridworld.obstacles:\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    return distances\n",
    "\n",
    "####### Goal Feature #######\n",
    "\n",
    "def goals_feature(traj):\n",
    "    \"\"\"\n",
    "    Compute goal feature values for all goals and the entire traj.\n",
    "    The goal feature consists of distance from the obstacle.\n",
    "    Params:\n",
    "        traj [list] -- The trajectory.\n",
    "    Returns:\n",
    "        goal_feat [list] -- The distance to the goals in the environment.\n",
    "    \"\"\"\n",
    "    goal_feat = np.zeros(len(gridworld.goals))\n",
    "    for s in traj:\n",
    "        goal_feat += np.asarray(dist_to_goals(s))\n",
    "    return goal_feat.tolist()\n",
    "\n",
    "def dist_to_goals(s):\n",
    "    \"\"\"\n",
    "    Compute distance from state s to the goal in the environment.\n",
    "    Params:\n",
    "        s [int] -- The state.\n",
    "    Returns:\n",
    "        distance [float] -- The distance to the goals in the environment.\n",
    "    \"\"\"\n",
    "    x, y = gridworld.state_to_coor(s)\n",
    "    distances = []\n",
    "\n",
    "    for goal in gridworld.goals:\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    return distances\n",
    "\n",
    "####### Coordinate Features #######\n",
    "\n",
    "def average_x_feature(traj):\n",
    "    \"\"\"\n",
    "    Compute average x feature value for the entire trajectory.\n",
    "    Params:\n",
    "        traj [list] -- The trajectory.\n",
    "    Returns:\n",
    "        avgx_feat [float] -- The average x feature value for entire traj.\n",
    "    \"\"\"\n",
    "    x_coords = [gridworld.state_to_coor(s)[0] for s in traj]\n",
    "    return np.mean(x_coords)\n",
    "\n",
    "def average_y_feature(traj):\n",
    "    \"\"\"\n",
    "    Compute average y feature value for the entire trajectory.\n",
    "    Params:\n",
    "        traj [list] -- The trajectory.\n",
    "    Returns:\n",
    "        avgy_feat [float] -- The average y feature value for entire traj.\n",
    "    \"\"\"\n",
    "    y_coords = [gridworld.state_to_coor(s)[1] for s in traj]\n",
    "    return np.mean(y_coords)\n",
    "\n",
    "\n",
    "####### Utils #######\n",
    "\n",
    "def featurize(traj, feat_list, scaling_coeffs=None):\n",
    "    \"\"\"\n",
    "    Computes the user-defined features for a given trajectory.\n",
    "    Params:\n",
    "        traj [list] -- A list of states the trajectory goes through.\n",
    "    Returns:\n",
    "        features [array] -- A list of feature values.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for feat in range(len(feat_list)):\n",
    "        if feat_list[feat] == 'goals':\n",
    "            features.extend(goals_feature(traj))\n",
    "        elif feat_list[feat] == 'obstacles':\n",
    "            features.extend(obstacles_feature(traj))\n",
    "        elif feat_list[feat] == 'avgx':\n",
    "            features.append(average_x_feature(traj))\n",
    "        elif feat_list[feat] == 'avgy':\n",
    "            features.append(average_y_feature(traj))\n",
    "    if scaling_coeffs is not None:\n",
    "        for feat in range(len(features)):\n",
    "            features[feat] = (features[feat] - scaling_coeffs[feat][\"min\"]) / (scaling_coeffs[feat][\"max\"] - scaling_coeffs[feat][\"min\"])\n",
    "    return np.asarray(features)\n",
    "\n",
    "def feat_scale_construct(feat_list):\n",
    "    \"\"\"\n",
    "    Construct scaling constants for the features available.\n",
    "    \"\"\"\n",
    "    # First featurize all trajectories with non-standard features.\n",
    "    Phi_nonstd = np.array([featurize(xi, feat_list) for xi in SG_trajs])\n",
    "\n",
    "    # Compute scaling coefficients depending on what feat_scaling is\n",
    "    scaling_coeffs = []\n",
    "    for Phi in Phi_nonstd.T:\n",
    "        min_val = min(Phi)\n",
    "        max_val = max(Phi)\n",
    "        coeffs = {\"min\": min_val, \"max\": max_val}\n",
    "        scaling_coeffs.append(coeffs)\n",
    "    return scaling_coeffs\n",
    "\n",
    "def visualize_feature(feat_vals, idx):\n",
    "    \"\"\"\n",
    "    Visualize the world with its obstacles and given demonstration.\n",
    "    \"\"\"\n",
    "    # Create world with obstacles on the map.\n",
    "    world = np.ones((gridworld.Y, gridworld.X))\n",
    "    for s in range(gridworld.S):\n",
    "        x, y = gridworld.state_to_coor(s)\n",
    "        world[y][x] = feat_vals[s][idx]\n",
    "\n",
    "    # Add obstacles in the world in opaque color.\n",
    "    for obstacle in gridworld.obstacles:\n",
    "        lower = obstacle[0]\n",
    "        upper = obstacle[1]\n",
    "        world[upper[1]:lower[1]+1, lower[0]:upper[0]+1] = 10.0\n",
    "\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    plt.imshow(world, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "    # Plot markers for start and goal\n",
    "    plt.scatter(gridworld.start[0], gridworld.start[1], c=\"orange\", marker=\"o\", s=100)\n",
    "    for goal in gridworld.goals:\n",
    "        plt.scatter(goal[0], goal[1], c=\"orange\", marker=\"x\", s=300)\n",
    "\n",
    "    plt.xticks(range(gridworld.X), range(gridworld.X))\n",
    "    plt.yticks(np.arange(-0.5,gridworld.Y+0.5),range(gridworld.Y+1))\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks([])\n",
    "    ax = plt.gca()\n",
    "    plt.minorticks_on\n",
    "    ax.grid(True, which='both', color='black', linestyle='-', linewidth=2)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize each feature to make sure your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Left obstacle. You should see the cells get darker the farther they are from the left obstacle.\n",
    "feat_list = [\"obstacles\"]\n",
    "state_feat_vals = [featurize([s], feat_list) for s in range(gridworld.S)]\n",
    "visualize_feature(state_feat_vals, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right obstacle. You should see the cells get darker the farther they are from the right obstacle.\n",
    "feat_list = [\"obstacles\"]\n",
    "state_feat_vals = [featurize([s], feat_list) for s in range(gridworld.S)]\n",
    "visualize_feature(state_feat_vals, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal. You should see the cells get darker the farther they are from the goal.\n",
    "feat_list = [\"goals\"]\n",
    "state_feat_vals = [featurize([s], feat_list) for s in range(gridworld.S)]\n",
    "visualize_feature(state_feat_vals, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Observation Model\n",
    "In class, we learned about the Maximum Entropy IRL observation model - the Boltzmann model - where trajectories are chosen in proportion to their exponentiated negative cost: $$P(\\xi\\mid\\theta,\\beta) = \\frac{e^{-\\beta\\theta^T\\Phi(\\xi)}}{\\int e^{-\\beta\\theta^T\\Phi(\\bar\\xi)}d\\bar\\xi} \\enspace .$$\n",
    "\n",
    "Here, $\\beta$ is an inverse temperature parameter that controls how rational or noisy the human is when giving demonstrations: high $\\beta$s produce demonstrations closer to optimal, while lower ones produce noisier demonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the Boltzmann noisy rationality model for a given $\\theta$ and $\\beta$. <b>Fill in the `observation_model` function below</b>, assuming that $\\Phi(\\xi)$ and $\\Phi(\\bar\\xi),\\forall \\bar\\xi\\in\\Xi$ are passed in already as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_model(Phi_xi, Phi_xibar, theta, beta):\n",
    "    \"\"\"\n",
    "    Finds observation model for given demonstrated features, using initialized model.\n",
    "    Params:\n",
    "        Phi_xi [array] -- The cost features for an observed trajectory.\n",
    "        Phi_xibar [list] -- A list of the cost features for all trajectories in the grid.\n",
    "        theta [list] -- The preference parameter.\n",
    "        beta [float] -- The rationality coefficient.\n",
    "    Returns:\n",
    "        P_xi_bt [float] -- P(xi | theta, beta)\n",
    "    \"\"\" \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    return P_xi_bt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the cost model and the observation model, the simulated human can now sample demonstrations. The function below does this by generating the feature vector for all $\\xi \\in \\Xi$, computing the Boltzmann probability for all of them, and sampling according to those probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_demonstrations(theta, beta, samples):\n",
    "    \"\"\"\n",
    "    Sample <samples> demonstrations for a given theta and beta.\n",
    "    Params:\n",
    "        theta [list] -- The preference parameter.\n",
    "        beta [float] -- The rationality coefficient.\n",
    "        samples [int] -- Number of demonstrations to be sampled.\n",
    "    \"\"\" \n",
    "    # Generate feature values for all trajectories in the gridworld.\n",
    "    Phi_xibar = [featurize(xi, feat_list, scaling_coeffs) for xi in SG_trajs]\n",
    "\n",
    "    # Create the xi observation model for all trajectories.\n",
    "    P_xi = [observation_model(Phi, Phi_xibar, theta, beta) for Phi in Phi_xibar]\n",
    "    \n",
    "    # Sample <samples> trajectories using this distribution.\n",
    "    traj_idx = np.random.choice(len(P_xi), samples, p=P_xi)\n",
    "\n",
    "    # Return trajectories given by traj_idx\n",
    "    return [SG_trajs[i] for i in traj_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create the simulated human and sample demonstrations. Let's say our human cares about the distances to the obstacles in the environment, and let's see how varying $\\theta$ and $\\beta$ changes their demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define parameters for simulated human.\n",
    "feat_list = [\"obstacles\"]\n",
    "num_features = 2\n",
    "scaling_coeffs = feat_scale_construct(feat_list) # Used for normalizing feature values.\n",
    "real_theta = np.array([1.0, 1.0])\n",
    "real_beta = 10.0\n",
    "num_demos = 10\n",
    "\n",
    "# Wants to stay close to both obstacles.\n",
    "demos = sample_demonstrations(real_theta, real_beta, num_demos)\n",
    "gridworld.visualize_demos(demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wants to stay close to left obstacle, doesn't care about the other one.\n",
    "demos = sample_demonstrations(np.array([1.0, 0.0]), 10.0, 100)\n",
    "gridworld.visualize_demos(demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wants to stay far from the left obstacle, close to the other one.\n",
    "demos = sample_demonstrations(np.array([-1.0, 1.0]), 10.0, 100)\n",
    "gridworld.visualize_demos(demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wants to stay far from the left obstacle, doesn't care about the other one.\n",
    "demos = sample_demonstrations(np.array([-1.0, 0.0]), 10.0, 100)\n",
    "gridworld.visualize_demos(demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Change the $\\beta$ parameter below</b> to create a human that almost always produces the optimal trajectory (pick a $\\beta$ that doesn't make the Python floating point system overflow, i.e. $|\\beta| \\leq 1000$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#near_optimal_beta = FILL IN\n",
    "demos = sample_demonstrations(np.array([1.0, 1.0]), near_optimal_beta, 100)\n",
    "gridworld.visualize_demos(demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now <b>change the $\\beta$ parameter below</b> to create a human that essentially picks trajectories randomly (pick a $\\beta$ that doesn't make the Python floating point system overflow, i.e. $|\\beta| \\leq 1000$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_beta = FILL IN\n",
    "demos = sample_demonstrations(np.array([1.0, 1.0]), random_beta, 100)\n",
    "gridworld.visualize_demos(demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from Demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some demonstrations from the simulated human, we want to infer just by observing their behavior how they chose that particular motion, i.e. what $\\theta$ and $\\beta$ parameters they used to generate demonstrations. This is the typical Inverse Reinforcement Learning (IRL) problem, which seeks to explain an observed demonstration by uncovering the demonstrator’s unknown objective function. We're going to implement Bayesian IRL to do so: $$P(\\theta, \\beta\\mid\\xi) = \\frac{P(\\xi\\mid\\theta,\\beta)P(\\theta, \\beta)}{\\int_{\\bar\\theta}\\int_{\\bar\\beta} P(\\xi\\mid\\bar\\theta,\\bar\\beta)P(\\bar\\theta, \\bar\\beta)d\\bar\\theta d\\bar\\beta} \\enspace.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the discussion in class, remember that Bayesian IRL is intractable because the double integral is infeasible to compute over continuous spaces. To make it tractable, we discretize the possible set of $\\theta$ and $\\beta$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference parameters\n",
    "theta_vals =  [-1.0, 0.0, 1.0]\n",
    "betas = [0.1, 0.3, 1.0, 3.0, 10.0, 30.0]\n",
    "thetas = list(iter.product(theta_vals, repeat=num_features))\n",
    "if (0.0,)*num_features in thetas:\n",
    "    thetas.remove((0.0,)*num_features)\n",
    "thetas = [w / np.linalg.norm(w) for w in thetas]\n",
    "thetas = set([tuple(i) for i in thetas])\n",
    "thetas = [list(i) for i in thetas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the discrete sets of possible $\\theta$ and $\\beta$ values, as well as the already computed feature vectors for $N$ demonstrations, <b>fill in the `inference` function below</b> to perform discrete Bayesian inference. We provide a uniform prior over $\\theta$ and $\\beta$ that you should use. \n",
    "\n",
    "Hint 1: the probability of multiple demonstrations is the product of the probability of each demonstration.\n",
    "\n",
    "Hint 2: use the `observation_model` function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(Phi_xis, thetas, betas):\n",
    "    \"\"\"\n",
    "    Performs inference from given demonstrated features, using initialized model.\n",
    "    Params:\n",
    "        Phi_xis [list] -- A list of the cost features for observed trajectories.\n",
    "        thetas [list] -- Possible theta vectors.\n",
    "        betas [list] -- Possible beta values.\n",
    "    Returns:\n",
    "        P_bt [array] -- Posterior probability P(beta, theta | xi_1...xi_N)\n",
    "    \"\"\"\n",
    "    prior = np.ones((len(betas), len(thetas))) / (len(betas) * len(thetas))\n",
    "    \n",
    "    # Generate feature values for all trajectories in the gridworld.\n",
    "    Phi_xibar = [featurize(xi, feat_list, scaling_coeffs) for xi in SG_trajs]\n",
    "\n",
    "    for b, beta in enumerate(betas):\n",
    "        for t, theta in enumerate(thetas):\n",
    "            # YOUR CODE HERE\n",
    "            pass\n",
    "           \n",
    "    return P_bt\n",
    "\n",
    "#################################\n",
    "#### Visualization functions ####\n",
    "#################################\n",
    "\n",
    "def visualize_posterior(prob, thetas, betas):\n",
    "    matplotlib.rcParams['font.sans-serif'] = \"Arial\"\n",
    "    matplotlib.rcParams['font.family'] = \"Times New Roman\"\n",
    "    matplotlib.rcParams.update({'font.size': 15})\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(prob, cmap='Blues', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.clim(0, None)\n",
    "\n",
    "    weights_rounded = [[round(i,2) for i in j] for j in thetas]\n",
    "    plt.xticks(range(len(thetas)), weights_rounded, rotation = 'vertical')\n",
    "    plt.yticks(range(len(betas)), betas)\n",
    "    plt.xlabel(r'$\\theta$')\n",
    "    plt.ylabel(r'$\\beta$')\n",
    "    plt.title(\"Joint Posterior Belief\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_marginal(marg, thetas):\n",
    "    matplotlib.rcParams['font.sans-serif'] = \"Arial\"\n",
    "    matplotlib.rcParams['font.family'] = \"Times New Roman\"\n",
    "    matplotlib.rcParams.update({'font.size': 15})\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow([marg], cmap='Oranges', interpolation='nearest')\n",
    "    plt.colorbar(ticks=[0, 0.5, 1.0])\n",
    "    plt.clim(0, 1.0)\n",
    "\n",
    "    weights_rounded = [[round(i,2) for i in j] for j in thetas]\n",
    "    plt.xticks(range(len(thetas)), weights_rounded, rotation = 'vertical')\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(r'$\\theta$')\n",
    "    plt.title(r'$\\theta$ Marginal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test your inference function. First, for a simulated human with high rationality coefficient $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define parameters for simulated human with high rationality, who wants to stay close to both obstacles.\n",
    "feat_list = [\"obstacles\"]\n",
    "num_features = 2\n",
    "scaling_coeffs = feat_scale_construct(feat_list) # Used for normalizing feature values.\n",
    "real_theta = np.array([1.0, 1.0])\n",
    "real_beta = 30.0\n",
    "num_demos = 10\n",
    "\n",
    "# Generate demonstrations.\n",
    "demos = sample_demonstrations(real_theta, real_beta, num_demos)\n",
    "\n",
    "# Generate feature values for all demonstrations.\n",
    "Phi_xis = [featurize(xi, feat_list, scaling_coeffs) for xi in demos]\n",
    "\n",
    "# Perform and visualize inference.\n",
    "posterior = inference(Phi_xis, thetas, betas)\n",
    "visualize_posterior(posterior, thetas, betas)\n",
    "visualize_marginal(sum(posterior), thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for a noisier human. Try running the following cell multiple times to see what happens. What do you notice?\n",
    "\n",
    "**Answer**: <font color=\"red\">YOUR ANSWER HERE</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate demonstrations with low rationality.\n",
    "demos = sample_demonstrations(real_theta, 0.1, num_demos)\n",
    "\n",
    "# Generate feature values for all demonstrations.\n",
    "Phi_xis = [featurize(xi, feat_list, scaling_coeffs) for xi in demos]\n",
    "\n",
    "# Perform and visualize inference.\n",
    "posterior = inference(Phi_xis, thetas, betas)\n",
    "visualize_posterior(posterior, thetas, betas)\n",
    "visualize_marginal(sum(posterior), thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to see how inference quality varies with the simulated human's rationality parameter $\\beta$. <b>Fill in `plot_inferred_with_beta`</b> to plot how $P(\\theta^*\\mid \\xi_1,...,\\xi_N)$, the marginal posterior probability of the true weight parameter $\\theta^*$, varies as the simulated human's $\\beta$ values change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inferred_with_beta(xs, num_sims, real_theta, num_demos):\n",
    "    \"\"\"\n",
    "    Plots P(theta^* | xi_1...xi_N) varies with beta.\n",
    "    Params:\n",
    "        xs [list] -- A list of the beta values for the simulated human.\n",
    "        num_sims [int] -- Number of different demo samplings to run per simulated human.\n",
    "        real_theta [list] -- True weight parameter theta^*.\n",
    "        num_demos [int] -- Number of demonstrations sampled for every simulation.\n",
    "    Returns:\n",
    "        P_bt [array] -- Posterior probability P(beta, theta | xi_1...xi_N)\n",
    "    \"\"\"\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        sims = []\n",
    "        # Use this to index into the marginal for the true theta.\n",
    "        true_idx = thetas.index((real_theta/np.linalg.norm(real_theta)).tolist()) \n",
    "        for _ in range(num_sims):\n",
    "            # YOUR CODE HERE\n",
    "            pass\n",
    "        ys.append(sims)\n",
    "    ys_mean = np.mean(ys, axis=1)\n",
    "    ys_std = stats.sem(ys, axis=1)\n",
    "    plt.errorbar(np.log(xs), ys_mean, ys_std, marker='o')\n",
    "    plt.xlabel(r'$log(\\beta$)')\n",
    "    plt.ylabel(r'$P(\\theta^*$)')\n",
    "    plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inferred_with_beta([0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0, 30.0, 100.0], 10, real_theta, num_demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to switch gears to intent inference. Instead of caring about obstacles, our agent is presented with 2 possible goals in the grid world. After seeing a partial trajectory $\\xi_{S\\rightarrow Q}$ from the start $S$ to an intermediate state $Q$, we want to compute the probability the trajectory is headed to either goal $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build gridworld.\n",
    "sim_height = 6\n",
    "sim_width = 8\n",
    "obstacles = [[[1,1], [1,1]] , [[4,3],[4,3]]]\n",
    "start = [3, 2]\n",
    "goals = [[7, 0], [7, 5]]\n",
    "feat_list = [\"goals\"]\n",
    "gridworld = AgentGridworld(sim_width, sim_height, obstacles, start, goals)\n",
    "gridworld.visualize_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is $P(G\\mid \\xi_{S\\rightarrow Q})$ for every goal $G\\in\\mathcal{G}$. From class, recall the following: $$P(G\\mid\\xi_{S\\rightarrow Q}) = \\frac{P(\\xi_{S\\rightarrow Q}\\mid G)P(G)}{\\sum_{\\bar G} P(\\xi_{S\\rightarrow Q}\\mid \\bar G)P(\\bar G)}\\enspace,$$ and $$P(\\xi_{S\\rightarrow Q}\\mid G) = \\frac{e^{-C_G(\\xi_{S\\rightarrow Q})}\\int_{\\xi_{Q\\rightarrow G}} e^{-C_G(\\xi_{Q\\rightarrow G})}d\\xi_{Q\\rightarrow G}}{\\int_{\\xi_{S\\rightarrow G}} e^{-C_G(\\xi_{S\\rightarrow G})}d\\xi_{S\\rightarrow G}}\\enspace.$$\n",
    "\n",
    "Let's define the cost function $C_G$ as the cumulative distance from the goal $G$. Using the above formulae, <b>implement the `goal_inference` function below</b>, which takes in the partial trajectory $\\xi_{S\\rightarrow Q}$ and the goal set $\\mathcal{G}$. We provide a uniform prior over goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_inference(traj, goals):\n",
    "    \"\"\"\n",
    "    Performs goal inference from given partial trajectory.\n",
    "    Params:\n",
    "        traj [list] -- The partial trajectory xi_SQ.\n",
    "        goals [list] -- List of goals.\n",
    "    Returns:\n",
    "        P_g [array] -- Posterior probability P(G | xi_SQ)\n",
    "    \"\"\"\n",
    "    prior = np.ones(len(goals)) / len(goals)\n",
    "    \n",
    "    Phi_xi = featurize(traj, feat_list) # Cost from S to Q, under both G1 and G2\n",
    "    for i in range(len(goals)):\n",
    "        SG_trajs = gridworld.traj_construct(start, goals[i])\n",
    "        QG_trajs = gridworld.traj_construct(gridworld.state_to_coor(traj[-1]), goals[i])\n",
    "        Phi_xiSG = [featurize(xi, feat_list)[i] for xi in SG_trajs] # Distances from S to G_i\n",
    "        Phi_xiQG = [featurize(xi, feat_list)[i] for xi in QG_trajs] # Distances from Q to G_i\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    return P_g\n",
    "\n",
    "\n",
    "def visualize_inference(prob, goals):\n",
    "    matplotlib.rcParams['font.sans-serif'] = \"Arial\"\n",
    "    matplotlib.rcParams['font.family'] = \"Times New Roman\"\n",
    "    matplotlib.rcParams.update({'font.size': 15})\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow([prob], cmap='Oranges', interpolation='nearest')\n",
    "    plt.colorbar(ticks=[0, 0.5, 1.0])\n",
    "    plt.clim(0, 1.0)\n",
    "\n",
    "    plt.xticks(range(len(goals)), goals, rotation = 'vertical')\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Goals')\n",
    "    plt.title(\"Inference\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your implementation for the following 3 partial trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = [(3,2), (4,2), (5,2)]\n",
    "demo = [gridworld.coor_to_state(x, y) for (x,y) in traj]\n",
    "gridworld.visualize_demos([demo])\n",
    "\n",
    "posterior = goal_inference(demo, goals)\n",
    "visualize_inference(posterior, goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = [(3,2), (4,2), (5,2), (5,1)]\n",
    "demo = [gridworld.coor_to_state(x, y) for (x,y) in traj]\n",
    "gridworld.visualize_demos([demo])\n",
    "\n",
    "posterior = goal_inference(demo, goals)\n",
    "visualize_inference(posterior, goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = [(3,2), (4,2), (5,2), (5,3)]\n",
    "demo = [gridworld.coor_to_state(x, y) for (x,y) in traj]\n",
    "gridworld.visualize_demos([demo])\n",
    "\n",
    "posterior = goal_inference(demo, goals)\n",
    "visualize_inference(posterior, goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expressing Intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been inferring what a person's intent is from their choice of trajectories. Now, we will reverse the roles and instead focus on the robot generating trajectories that express its intent to the human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build grid world.\n",
    "sim_width = 8\n",
    "sim_height = 6\n",
    "obstacles = [[[1,1], [1,1]] , [[4,3],[4,3]]]\n",
    "start = [3, 2]\n",
    "goals = [[7, 0], [7, 5]]\n",
    "feat_list = [\"goals\"]\n",
    "gridworld = AgentGridworld(sim_width, sim_height, obstacles, start, goals)\n",
    "gridworld.visualize_grid()\n",
    "\n",
    "# Build possible trajectories for each goal.\n",
    "SG_trajs = [gridworld.traj_construct(start, goals[0]), gridworld.traj_construct(start, goals[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above grid world, we want to produce the robot trajectory that maximizes *predictability* to either goal. That is, we want to produce the trajectory that is most likely for a particular goal: $$\\xi^{pred} = \\max_{\\xi} P(\\xi\\mid G) = \\frac{e^{-C_G(\\xi)}}{\\int e^{-C_G(\\bar\\xi)}d\\bar\\xi} \\enspace .$$ <b>Fill in the `predictable_trajectory` function below</b>, that maximizes predictability with respect to a goal given by `goal_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictable_trajectory(goal_idx):\n",
    "    \"\"\"\n",
    "    Compute trajectory that maximizes predictability.\n",
    "    Params:\n",
    "        goal_idx [int] -- The goal w.r.t. we want to maximize predictability\n",
    "    Returns:\n",
    "        pred_traj [list] -- The trajectory that maximizes predictability.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Generate feature values for all trajectories in the gridworld.\n",
    "    Phi_xiSG = [featurize(xi, feat_list)[goal_idx] for xi in SG_trajs[goal_idx]]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # You want to get the index pred_idx of the most predictable trajectory.\n",
    "    pass\n",
    "\n",
    "    pred_traj = SG_trajs[goal_idx][pred_idx]\n",
    "    return pred_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test your code below for each goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = predictable_trajectory(0)\n",
    "gridworld.visualize_demos([traj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = predictable_trajectory(1)\n",
    "gridworld.visualize_demos([traj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we also discussed robot trajectories that are *legible* for an intent. Legible trajectories are trajectories such that the person would easily be able to distinguish the robot's intent early on: $$\\xi^{leg} = \\max_{\\xi} P(G\\mid \\xi) = \\frac{P(\\xi\\mid G)P(G)}{\\sum_{\\bar{G}} P(\\xi\\mid \\bar{G})P(\\bar{G})} \\enspace .$$ <b>Fill in the `legible_trajectory` function below</b>, that maximizes legibility with respect to a goal given by `goal_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legible_trajectory(goals, goal_idx):\n",
    "    \"\"\"\n",
    "    Compute trajectory that maximizes legibility.\n",
    "    Params:\n",
    "        goals [list] -- List of goals in the environment.\n",
    "        goal_idx [int] -- The goal w.r.t. we want to maximize predictability\n",
    "    Returns:\n",
    "        leg_traj [list] -- The trajectory that maximizes predictability.\n",
    "    \"\"\"\n",
    "    # max_xi P(g | xi)\n",
    "    prior = np.ones(len(goals)) / len(goals)\n",
    "    \n",
    "    Phi_xiSGs = []\n",
    "    C_xiSGs = []\n",
    "    for i in range(len(goals)):\n",
    "        # Generate feature values for all trajectories in the gridworld.\n",
    "        Phi_xiSG = [featurize(xi, feat_list)[i] for xi in SG_trajs[i]]\n",
    "        C_xiSG = [-Phi for Phi in Phi_xiSG]\n",
    "        Phi_xiSGs.append(Phi_xiSG)\n",
    "        C_xiSGs.append(C_xiSG)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # You want to get the index leg_idx of the most legible trajectory.\n",
    "    pass\n",
    "\n",
    "    leg_traj = SG_trajs[goal_idx][leg_idx]\n",
    "    return leg_traj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test your code below for each goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = legible_trajectory(goals, 0)\n",
    "gridworld.visualize_demos([traj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = legible_trajectory(goals, 1)\n",
    "gridworld.visualize_demos([traj])"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
